name: Update cars snapshot (chunked)

on:
  schedule:
    - cron: '15 5 * * *'   # daily 05:15 UTC
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Build chunked snapshot (index + cars_####.json)
        run: |
          set -euo pipefail

          BASE="https://vehicle-api-espm.onrender.com/cars"
          PER_PAGE=20000
          MAX_PAGES=40
          T="${{ github.run_id }}"

          rm -f cars.json cars_raw.json page_*.json cars_*.json cars_index.json

          echo '{"cars":[]}' > cars_raw.json

          fetch_page () {
            local url="$1"
            local out="$2"
            local attempt=1

            while [ "$attempt" -le 8 ]; do
              code="$(curl -sSLo "$out" \
                -H 'Cache-Control: no-cache' \
                -H 'Accept: application/json' \
                --compressed \
                --connect-timeout 20 \
                --max-time 240 \
                -w "%{http_code}" \
                "$url" || true)"

              if [ "$code" = "200" ]; then
                return 0
              fi

              if [ "$code" = "502" ] || [ "$code" = "503" ] || [ "$code" = "504" ]; then
                echo "::warning ::HTTP $code (attempt $attempt/8). Backing off..."
                sleep $((attempt * 10))
                attempt=$((attempt+1))
                continue
              fi

              echo "::error ::HTTP $code from API"
              head -c 2000 "$out" || true
              return 1
            done

            echo "::error ::API kept returning 502/503/504 after retries"
            head -c 2000 "$out" || true
            return 1
          }

          # build chunk files directly (no giant cars.json)
          files=()
          total=0

          page=1
          while [ "$page" -le "$MAX_PAGES" ]; do
            url="${BASE}?lite=1&include_meta=0&per_page=${PER_PAGE}&page=${page}&_t=${T}"
            raw="page_${page}.json"
            echo "Downloading page ${page}: $url"

            fetch_page "$url" "$raw"

            # validate wrapper
            if ! jq -e 'has("cars") and (.cars | type=="array")' "$raw" >/dev/null; then
              echo "::error ::Page ${page} missing { cars: [] } wrapper"
              jq -r 'keys | join(", ")' "$raw" || true
              exit 1
            fi

            chunk="cars_$(printf '%04d' $page).json"

            # normalize this page into a chunk array
            jq -c '
              (.cars // [])
              | map(
                  . as $c
                  | $c + {
                      lat: ($c.latitude // $c.lat),
                      lng: ($c.longitude // $c.lng),
                      price: ($c.price_gbp // $c.price),
                      mileage: ($c.mileage_mi // $c.mileage),
                      fuel: ($c.fuel_type // $c.fuel),
                      gearbox: ($c.transmission // $c.gearbox),
                      bodytype: ($c.body_type // $c.bodytype),
                      dealer: ($c.dealer_name // $c.dealer),
                      url: ($c.listing_url // $c.url),
                      thumb: ($c.image_cover_url // $c.thumb),

                      owners: ($c.num_owners // $c.owners),
                      reg: ($c.vehicle_registration_mark // $c.reg),
                      maxspeed_mph: ($c.performance_maxspeed_mph // $c.maxspeed_mph),
                      zero_to_60_mph: ($c.performance_acceleration_zero_to_60_mph // $c.zero_to_60_mph),
                      combined_mpg: ($c.efficiency_combined_mpg // $c.combined_mpg),

                      engine: ($c.engine_size_l // $c.engine),
                      power: ($c.power_ps // $c.power),
                      euro: ($c.euro_standard // $c.euro),
                      colour: ($c.color // $c.colour),

                      images_count: ($c.images_count // 0),
                      options: ($c.options // []),
                      features: ($c.features // []),
                      seller_comments: ($c.seller_comments // "")
                    }
                )
            ' "$raw" > "$chunk"

            n="$(jq 'length' "$chunk")"
            echo "Wrote $chunk with $n cars"
            total=$((total + n))
            files+=("$chunk")

            # stop when last page is short
            if [ "$n" -lt "$PER_PAGE" ]; then
              break
            fi

            page=$((page+1))
            sleep 2
          done

          ts="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
          printf '%s\n' "${files[@]}" | jq -R -s -c 'split("\n")[:-1]' > _files.json

          jq -n \
            --arg ts "$ts" \
            --argjson files "$(cat _files.json)" \
            --argjson total "$total" \
            --argjson chunks "$(printf '%s\n' "${#files[@]}" | jq -R 'tonumber')" \
            '{generated_at:$ts,total_cars:$total,chunks:$chunks,files:$files}' > cars_index.json

          rm -f _files.json page_*.json cars_raw.json

          echo "Index:"
          cat cars_index.json
          echo "Files:"
          ls -lh cars_index.json cars_*.json

      - name: Commit & push snapshot files
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail

          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Ensure old monolithic file is removed if it exists in repo
          git rm -f --ignore-unmatch cars.json

          # Add new snapshot artifacts
          git add cars_index.json cars_*.json

          # Commit if there are changes
          if git diff --cached --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Daily snapshot (chunked)"
          git push
