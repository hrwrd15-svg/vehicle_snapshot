name: Update cars snapshot (chunked)

on:
  schedule:
    - cron: '15 5 * * *'   # daily 05:15 UTC
  workflow_dispatch:

jobs:
  update:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - uses: actions/checkout@v4

      - name: Fetch cars JSON (paged + lite + no-meta + backoff)
        run: |
          set -euo pipefail

          BASE="https://vehicle-api-espm.onrender.com/cars"
          PER_PAGE=20000
          MAX_PAGES=40
          T="${{ github.run_id }}"

          rm -f cars_raw.json cars.json page_*.json cars_*.json cars_index.json

          echo '{"cars":[]}' > cars_raw.json

          fetch_page () {
            local url="$1"
            local out="$2"

            local attempt=1
            while [ "$attempt" -le 8 ]; do
              local code
              code="$(
                curl -sSLo "$out" \
                  -H 'Cache-Control: no-cache' \
                  -H 'Accept: application/json' \
                  --compressed \
                  --connect-timeout 20 \
                  --max-time 240 \
                  -w "%{http_code}" \
                  "$url" || true
              )"

              if [ "$code" = "200" ]; then
                return 0
              fi

              if [ "$code" = "503" ] || [ "$code" = "502" ] || [ "$code" = "504" ]; then
                echo "::warning ::HTTP $code (attempt $attempt/8). Backing off..."
                case "$attempt" in
                  1) sleep 5 ;;
                  2) sleep 10 ;;
                  3) sleep 20 ;;
                  4) sleep 30 ;;
                  5) sleep 45 ;;
                  6) sleep 60 ;;
                  7) sleep 90 ;;
                  8) sleep 120 ;;
                esac
                attempt=$((attempt+1))
                continue
              fi

              echo "::error ::HTTP $code from API"
              echo "::error ::Response (first 2000 chars):"
              head -c 2000 "$out" || true
              return 1
            done

            echo "::error ::API kept returning 502/503/504 after multiple retries."
            echo "::error ::Last response (first 2000 chars):"
            head -c 2000 "$out" || true
            return 1
          }

          page=1
          while [ "$page" -le "$MAX_PAGES" ]; do
            URL="${BASE}?lite=1&include_meta=0&per_page=${PER_PAGE}&page=${page}&_t=${T}"
            OUT="page_${page}.json"
            echo "Downloading page ${page}: $URL"

            fetch_page "$URL" "$OUT"

            if ! jq -e '.' "$OUT" >/dev/null 2>&1; then
              echo "::error ::Page ${page} is not valid JSON"
              head -c 2000 "$OUT" || true
              exit 1
            fi

            if ! jq -e 'has("cars") and (.cars | type=="array")' "$OUT" >/dev/null; then
              echo "::error ::Page ${page} missing { cars: [] } wrapper"
              jq -r 'keys | join(", ")' "$OUT" || true
              exit 1
            fi

            count="$(jq '.cars | length' "$OUT")"
            echo "Page ${page} cars: ${count}"

            jq -s '.[0].cars += .[1].cars | .[0]' cars_raw.json "$OUT" > cars_raw_merged.json
            mv cars_raw_merged.json cars_raw.json

            sleep 2

            if [ "$count" -lt "$PER_PAGE" ]; then
              break
            fi

            page=$((page+1))
          done

          total="$(jq '.cars | length' cars_raw.json)"
          echo "cars_raw.json total cars: $total"
          if [ "$total" -lt 1 ]; then
            echo "::error ::No cars returned"
            exit 1
          fi

      - name: Normalize snapshot (array + legacy aliases + defaults)
        run: |
          set -euo pipefail

          jq -c '
            (.cars // [])
            | map(
                . as $c
                | $c
                + {
                    lat: ($c.latitude // $c.lat),
                    lng: ($c.longitude // $c.lng),
                    price: ($c.price_gbp // $c.price),
                    mileage: ($c.mileage_mi // $c.mileage),
                    fuel: ($c.fuel_type // $c.fuel),
                    gearbox: ($c.transmission // $c.gearbox),
                    bodytype: ($c.body_type // $c.bodytype),
                    dealer: ($c.dealer_name // $c.dealer),
                    url: ($c.listing_url // $c.url),
                    thumb: ($c.image_cover_url // $c.thumb),

                    owners: ($c.num_owners // $c.owners),
                    reg: ($c.vehicle_registration_mark // $c.reg),
                    maxspeed_mph: ($c.performance_maxspeed_mph // $c.maxspeed_mph),
                    zero_to_60_mph: ($c.performance_acceleration_zero_to_60_mph // $c.zero_to_60_mph),
                    combined_mpg: ($c.efficiency_combined_mpg // $c.combined_mpg),

                    engine: ($c.engine_size_l // $c.engine),
                    power: ($c.power_ps // $c.power),
                    euro: ($c.euro_standard // $c.euro),
                    colour: ($c.color // $c.colour),

                    images_count: ($c.images_count // 0),
                    options: ($c.options // []),
                    features: ($c.features // []),
                    seller_comments: ($c.seller_comments // "")
                  }
              )
          ' cars_raw.json > cars.json

          n="$(jq 'length' cars.json)"
          echo "Normalized cars.json length: $n"
          echo "cars.json size: $(wc -c < cars.json) bytes"

      - name: Chunk cars.json into <95MB files + write index
        run: |
          set -euo pipefail

          # Target max chunk size (bytes). Keep margin under 100MB hard limit.
          MAX_BYTES=$((95 * 1024 * 1024))

          rm -f cars_*.json cars_index.json

          # Stream items and build chunks by approximate size
          idx=1
          bytes=2   # "[]"
          echo "[" > "cars_$(printf '%04d' $idx).json"

          # Iterate each compact item
          jq -c '.[]' cars.json | while IFS= read -r line; do
            # +1 for comma/newline
            lbytes=$(printf '%s' "$line" | wc -c | tr -d ' ')
            add=$((lbytes + 2))

            if [ "$bytes" -gt 2 ] && [ $((bytes + add)) -gt "$MAX_BYTES" ]; then
              # close current chunk
              echo "]" >> "cars_$(printf '%04d' $idx).json"
              idx=$((idx+1))
              bytes=2
              echo "[" > "cars_$(printf '%04d' $idx).json"
            fi

            if [ "$bytes" -gt 2 ]; then
              echo "," >> "cars_$(printf '%04d' $idx).json"
              bytes=$((bytes+1))
            fi

            printf '%s' "$line" >> "cars_$(printf '%04d' $idx).json"
            bytes=$((bytes + lbytes))
          done

          # close last chunk
          echo "]" >> "cars_$(printf '%04d' $idx).json"

          # build index
          files=$(ls -1 cars_*.json | sort | jq -R -s -c 'split("\n")[:-1]')
          total=$(jq 'length' cars.json)
          chunks=$(ls -1 cars_*.json | wc -l | tr -d ' ')
          ts=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

          jq -n \
            --argjson files "$files" \
            --argjson total "$total" \
            --argjson chunks "$chunks" \
            --arg ts "$ts" \
            '{generated_at:$ts,total_cars:$total,chunks:$chunks,files:$files}' \
            > cars_index.json

          echo "Wrote $chunks chunk files."
          ls -lh cars_*.json cars_index.json || true

          # Safety: fail if any chunk exceeds 99MB
          too_big=$(find . -maxdepth 1 -name 'cars_*.json' -size +99M | wc -l | tr -d ' ')
          if [ "$too_big" != "0" ]; then
            echo "::error ::One or more chunk files exceeded 99MB. Adjust MAX_BYTES lower."
            find . -maxdepth 1 -name 'cars_*.json' -size +99M -print -exec ls -lh {} \;
            exit 1
          fi

      - name: Remove monolithic cars.json (avoid push rejection)
        run: |
          set -euo pipefail
          rm -f cars.json cars_raw.json page_*.json || true

      - name: Commit if changed
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Daily snapshot (chunked)"
          file_pattern: |
            cars_index.json
            cars_*.json
